{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "A Spark Cluster with access to the previous made hdfs cluster. \n",
    "\n",
    "(add topology diagram)\n",
    "\n",
    "Details on the HDFS cluster: \n",
    "- [09-hdfs-on-arm-pi/run-book-hdfs-on-pi.ipynb](https://github.com/6za/runbooks-jupyter/blob/master/09-hdfs-on-arm-pi/run-book-hdfs-on-pi.ipynb)\n",
    "\n",
    "\n",
    "This cluster has:\n",
    "- 1 Spark Master Node\n",
    "- 2 Spark Workers Node\n",
    "- 1 Spark Driver/App Node\n",
    "\n",
    "The solution is based on the docker-compose files at [6za/spark-sample](https://github.com/6za/spark-sample).\n",
    "\n",
    "The files are enhanced by setup to add network to allow this solution to work over multiple docker hosts without meshing them into a single virtual network. Exploring some Spark configs to allow pre-defined port allocation. \n",
    "\n",
    "This setup is using Intel Servers on this inital deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Links\n",
    "- HDFS name node\n",
    "- Spark master node\n",
    "- Spark driver node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = pd.read_csv(\"../common/hosts.csv\")\n",
    "supressed_columns = ['ip','user']\n",
    "raspi_name_hosts = hosts[(hosts.hostname == \"pi-node8\")]\n",
    "spark_master_hosts = hosts[(hosts.hostname == \"kx-Lenovo-H520g\")]\n",
    "spark_driver_hosts = hosts[(hosts.hostname == \"nuc-01\")]\n",
    "spark_workers_hosts = hosts[(hosts.hostname == \"nuc-02\") | (hosts.hostname == \"nuc-03\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pi-node8</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hostname    arch  gpu\n",
       "11  pi-node8  armv7l    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raspi_name_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kx-Lenovo-H520g</td>\n",
       "      <td>x86_64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hostname    arch  gpu\n",
       "3  kx-Lenovo-H520g  x86_64    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_master_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nuc-01</td>\n",
       "      <td>x86_64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hostname    arch  gpu\n",
       "4   nuc-01  x86_64    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_driver_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nuc-02</td>\n",
       "      <td>x86_64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nuc-03</td>\n",
       "      <td>x86_64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hostname    arch  gpu\n",
       "5   nuc-02  x86_64    0\n",
       "6   nuc-03  x86_64    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_workers_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/root/repos': File exists\n",
      "Cloning into 'spark-sample'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir ~/repos\n",
    "cd ~/repos\n",
    "rm -rf ~/repos/spark-sample\n",
    "git clone https://github.com/6za/spark-sample.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create host lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Host List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "namenode_ip = raspi_name_hosts.iloc[0]['ip']\n",
    "namenode_config = 'hadoop-namenode:%(ip)s' %  {\"ip\": namenode_ip}\n",
    "spark_master_ip = spark_master_hosts.iloc[0]['ip']\n",
    "spark_master_config = 'spark-master:%(ip)s' %  {\"ip\": spark_master_ip}\n",
    "driver_hosts_list = [namenode_config,spark_master_config]\n",
    "\n",
    "len(driver_hosts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Hosts List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_driver_ip = spark_driver_hosts.iloc[0]['ip']\n",
    "spark_driver_config = 'spark-driver:%(ip)s' %  {\"ip\": spark_driver_ip}\n",
    "\n",
    "worker_hosts_list = [namenode_config,spark_master_config,spark_driver_config]\n",
    "\n",
    "len(worker_hosts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Spark Master\n",
    "\n",
    "- Deploy as is no modification on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mkx-Lenovo-H520g\u001b[0m\n",
      "Spark Master Started\n"
     ]
    }
   ],
   "source": [
    "for index, row in spark_master_hosts.iterrows():\n",
    "    print('\\x1b[1;35m'+ row['hostname']+'\\x1b[0m')\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker-compose -f ~/repos/spark-sample/docker-compose-spark-master.yaml down\"  \n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "    command = \"docker-compose -f ~/repos/spark-sample/docker-compose-spark-master.yaml up -d\"  \n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "    print(\"Spark Master Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Spark Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mnuc-02\u001b[0m\n",
      "  Spark Worker started:nuc-02\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                                    NAMES\n",
      "d130f28621cf        jupyter/all-spark-notebook   \"/usr/local/spark/bi…\"   7 seconds ago       Up 5 seconds        0.0.0.0:4040->4040/tcp, 0.0.0.0:51814-51815->51814-51815/tcp, 8888/tcp   11-spark-cluster_spark-worker_1\n",
      "e599fdae0217        amkay/sensor-exporter        \"/go/bin/sensor-expo…\"   3 weeks ago         Up 30 hours         0.0.0.0:9255->9255/tcp                                                   node-exporter-collectors_tempsensor_1\n",
      "b2424bf443eb        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   3 weeks ago         Up 30 hours         0.0.0.0:9100->9100/tcp                                                   nodeexporter\n",
      "\n",
      "\u001b[1;35mnuc-03\u001b[0m\n",
      "  Spark Worker started:nuc-03\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                                      NAMES\n",
      "47d6ccd6f459        jupyter/all-spark-notebook   \"/usr/local/spark/bi…\"   7 seconds ago       Up 5 seconds        0.0.0.0:4040->4040/tcp, 0.0.0.0:51814-51815->51814-51815/tcp, 8888/tcp     11-spark-cluster_spark-worker_1\n",
      "dbf4365e5039        6zar/pyspark_docker          \"/bin/sh -c 'cd /roo…\"   3 weeks ago         Up 30 hours         0.0.0.0:8888->8888/tcp                                                     runbooks\n",
      "fe1bee3b22f6        amkay/sensor-exporter        \"/go/bin/sensor-expo…\"   3 weeks ago         Up 30 hours         0.0.0.0:9255->9255/tcp                                                     node-exporter-collectors_tempsensor_1\n",
      "f941a7dc93e0        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   3 weeks ago         Up 30 hours         0.0.0.0:9100->9100/tcp                                                     nodeexporter\n",
      "3339d746d6a7        consul:latest                \"docker-entrypoint.s…\"   4 months ago        Up 30 hours         8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8500->8500/tcp   consul_dev-consul_1\n",
      "c817dc67b992        nginx                        \"nginx -g 'daemon of…\"   4 months ago        Up 30 hours         0.0.0.0:80->80/tcp                                                         nginx\n",
      "7f7f98f6ac85        couchdb                      \"tini -- /docker-ent…\"   4 months ago        Up 30 hours         0.0.0.0:4369->4369/tcp, 0.0.0.0:5984->5984/tcp, 0.0.0.0:9101->9100/tcp     cluster-manager_couchdb_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "node_count = 0\n",
    "with open('/root/repos/spark-sample/docker-compose-spark-worker.yaml',\"r\") as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    config['services']['spark-worker']['extra_hosts'] = worker_hosts_list\n",
    "    for index, row in spark_workers_hosts.iterrows():\n",
    "        node_count = node_count + 1\n",
    "        config['services']['spark-worker']['hostname'] = 'spark-worker-' + row['hostname']\n",
    "        with open(r'docker-compose-spark-worker-current.yaml', 'w') as outputfile:\n",
    "            documents = yaml.dump(config, outputfile)    \n",
    "        print('\\x1b[1;35m'+ row['hostname']+'\\x1b[0m')\n",
    "        docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "        command = \"docker-compose -f docker-compose-spark-worker-current.yaml down\"\n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        command = \"docker-compose -f docker-compose-spark-worker-current.yaml up -d\"  \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(\"  Spark Worker started:\" + row['hostname'])\n",
    "        !sleep 5\n",
    "        command = \"docker ps\"  \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(result)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Spark Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mnuc-01\u001b[0m\n",
      "  Spark Driver started:nuc-01\n",
      "Set username to: jovyan\n",
      "Executing the command: jupyter notebook\n",
      "\n"
     ]
    }
   ],
   "source": [
    "node_count = 0\n",
    "with open('/root/repos/spark-sample/docker-compose-spark-jupyter.yaml',\"r\") as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    config['services']['spark-driver']['extra_hosts'] = driver_hosts_list\n",
    "    for index, row in spark_driver_hosts.iterrows():\n",
    "        node_count = node_count + 1\n",
    "        #fix_env = config['services']['spark-driver']['environment'].append\n",
    "        config['services']['spark-driver']['environment'].append(\"SPARK_PUBLIC_DNS=%(ip)s\" % {\"ip\": row['ip']})\n",
    "        with open(r'docker-compose-spark-jupyter-current.yaml', 'w') as outputfile:\n",
    "            documents = yaml.dump(config, outputfile)    \n",
    "        print('\\x1b[1;35m'+ row['hostname']+'\\x1b[0m')\n",
    "        docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "        command = \"docker-compose -f docker-compose-spark-jupyter-current.yaml down\"\n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        command = \"docker-compose -f docker-compose-spark-jupyter-current.yaml up -d\"  \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(\"  Spark Driver started:\" + row['hostname'])\n",
    "        !sleep 20\n",
    "        command = \"docker logs 11-spark-cluster_spark-driver_1\"  \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Driver app:\n",
    "\n",
    "Create via: http://`$spark-driver-ip`:8888/\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark1 = SparkSession.builder.appName('Sample').config(\"spark.driver.port\", 51816).config(\"spark.blockManager.port\", 51814).master('spark://spark-master:7077').getOrCreate()\n",
    "spark1.sparkContext._conf.getAll()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "range100 = spark1.range(100).withColumn(\"TEST\",lit(\"1fdgs;dlkfg;lsdkgkf\")).withColumn(\"TEST2\",lit(\"4dsklgdfs;kg;lsdkfl;hk\"))\n",
    "\n",
    "range100.show()\n",
    "\n",
    "range100.write.mode('overwrite').parquet(\"hdfs://hadoop-namenode:9000/sample1.txt\")\n",
    "\n",
    "\n",
    "sampleRead = spark1.read.parquet(\"hdfs://hadoop-namenode:9000/sample1.txt\")\n",
    "sampleRead.show()\n",
    "```\n",
    "\n",
    "Check cluster health at: http://`$spark-master-ip`:8088/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
