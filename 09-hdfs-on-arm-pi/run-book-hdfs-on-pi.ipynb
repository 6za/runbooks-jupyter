{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "This run books automates the deployment of HDFS Cluster in arms raspberry hosts using docker. \n",
    "\n",
    "A key aspect of this example is that all HDFS nodes are in different hosts, and that requires some adjustments on the docker-compose files to allow all nodes to find the other nodes to allow the cluster to work as expected. This is done without using a external DNS service, just docker compose parameters. \n",
    "\n",
    "\n",
    "## Original Topology:\n",
    "\n",
    "![HDFS Topology](./HDFS-Cluster-Topology.png \"Toplogy\")\n",
    "\n",
    "The elements:\n",
    "\n",
    "- 1 Master Node\n",
    "- 2 Data Nodes\n",
    "- 1 HDFS Client Apllication \n",
    "\n",
    "\n",
    "`/home/%(user)s/docker_host/hdfs` is the folder for the files of the images.\n",
    "\n",
    "Based on repo: https://github.com/6za/hdfs-sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hosts roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = pd.read_csv(\"../common/hosts.csv\")\n",
    "supressed_columns = ['ip','user']\n",
    "\n",
    "\n",
    "raspi_hosts = hosts[(hosts.hostname == \"pi-node8\") | (hosts.hostname == \"pi-node7\") | (hosts.hostname == \"pi-node5\") | (hosts.hostname == \"pi-node4\") ]\n",
    "raspi_hosts['ip'].to_csv(r'raspi-ip.txt', header=False, index=None, sep=' ')\n",
    "\n",
    "\n",
    "i386_hosts = hosts[(hosts.hostname == \"nuc-03\") ]\n",
    "i386_hosts['ip'].to_csv(r'i386-ip.txt', header=False, index=None, sep=' ')\n",
    "\n",
    "\n",
    "raspi_name_hosts = hosts[(hosts.hostname == \"pi-node8\")]\n",
    "# Format name nodes - Destructive action\n",
    "format_namenode = True\n",
    "\n",
    "# Clean data nodes - Destructive action\n",
    "clean_datanode = True\n",
    "\n",
    "raspi_data_hosts = hosts[(hosts.hostname == \"pi-node7\") | (hosts.hostname == \"pi-node5\")]\n",
    "\n",
    "\n",
    "\n",
    "raspi_test_hosts = hosts[(hosts.hostname == \"pi-node4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nuc-03</td>\n",
       "      <td>x86_64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hostname    arch  gpu\n",
       "6   nuc-03  x86_64    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i386_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pi-node4</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pi-node5</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pi-node7</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pi-node8</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hostname    arch  gpu\n",
       "7   pi-node4  armv7l    0\n",
       "8   pi-node5  armv7l    0\n",
       "10  pi-node7  armv7l    0\n",
       "11  pi-node8  armv7l    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raspi_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pi-node8</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hostname    arch  gpu\n",
       "11  pi-node8  armv7l    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raspi_name_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pi-node4</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hostname    arch  gpu\n",
       "7  pi-node4  armv7l    0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raspi_test_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>arch</th>\n",
       "      <th>gpu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pi-node5</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pi-node7</td>\n",
       "      <td>armv7l</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hostname    arch  gpu\n",
       "8   pi-node5  armv7l    0\n",
       "10  pi-node7  armv7l    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raspi_data_hosts.drop(columns=supressed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/root/repos': File exists\n",
      "Cloning into 'hdfs-sample'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir ~/repos\n",
    "cd ~/repos\n",
    "rm -rf ~/repos/hdfs-sample\n",
    "git clone https://github.com/6za/hdfs-sample.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Image on Nodes/Hosts\n",
    "\n",
    "This step could be replaced by having a local/public image registry. For this example we build the image on each host by design of this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image rm at pi-node4\n",
      "Image rm at pi-node5\n",
      "Image rm at pi-node7\n",
      "Image rm at pi-node8\n",
      "Image Built at pi-node4local-hadoop         3.2.1               ca6589c3b5de        22 hours ago        1.35GB\n",
      "\n",
      "Image Built at pi-node5local-hadoop         3.2.1               5c6a8d08da69        22 hours ago        1.35GB\n",
      "\n",
      "Image Built at pi-node7local-hadoop         3.2.1               c7d7062b8fd9        22 hours ago        1.35GB\n",
      "\n",
      "Image Built at pi-node8local-hadoop         3.2.1               5a08de6c74e7        22 hours ago        1.35GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm build_log.txt\n",
    "for index, row in raspi_hosts.iterrows():\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker rmi local-hadoop:3.2.1 \"\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print(\"Image rm at \"+ row['hostname'] + result )\n",
    "    \n",
    "for index, row in raspi_hosts.iterrows():\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker build --file ~/repos/hdfs-sample/Dockerfile.armhf ~/repos/hdfs-sample/ -t local-hadoop:3.2.1 >> build_log.txt\"\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    command = \"docker images | grep  local-hadoop \"\n",
    "    images_list = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print(\"Image Built at \"+ row['hostname'] + images_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update datanode compose with with namenode address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SSH Key\n",
    "\n",
    "Main Command: `ssh-keygen -q -t rsa -N '' -f id_rsa`\n",
    "\n",
    "> An `i386` image/host is used for compatibility and speed to generate the certs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in i386_hosts.iterrows():\n",
    "    command_mk = \"ssh %(user)s@%(ip)s mkdir -p /var/tmp/certs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_mk = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_mk)  , shell=True, encoding='utf-8')\n",
    "    command = \"docker pull 6zar/ssh_tool && docker run -v /var/tmp/certs:/root 6zar/ssh_tool | grep identification\"\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  2>&1 || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    command_cp = \"scp %(user)s@%(ip)s:/var/tmp/certs/* .\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_cp = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_cp)  , shell=True, encoding='utf-8')\n",
    "    !cat ./id_rsa.pub > ./authorized_keys\n",
    "    command_rm = \"ssh %(user)s@%(ip)s rm -rf /var/tmp/certs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_rm = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_rm)  , shell=True, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Keys on all nodes\n",
    "\n",
    "Accordingly with some HDFS stackoverflow this is required to allow nodes to exchange files.\n",
    "\n",
    "This will copy `id_rsa*` files and `authorized_keys` keys to all hosts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys deployed at pi-node4\n",
      "Keys deployed at pi-node5\n",
      "Keys deployed at pi-node7\n",
      "Keys deployed at pi-node8\n"
     ]
    }
   ],
   "source": [
    "for index, row in raspi_hosts.iterrows():\n",
    "    command_rmdir = \"ssh %(user)s@%(ip)s rm -rf /home/%(user)s/docker_host/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_rmdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_rmdir)  , shell=True, encoding='utf-8')\n",
    "    command_mkdir = \"ssh %(user)s@%(ip)s mkdir -p /home/%(user)s/docker_host/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_mkdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_mkdir)  , shell=True, encoding='utf-8')\n",
    "    # Copy idrsa\n",
    "    command_cp = \"scp ./id_rsa*  %(user)s@%(ip)s:/home/%(user)s/docker_host/hdfs/\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_cp = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_cp)  , shell=True, encoding='utf-8')\n",
    "    command_cp = \"scp ./authorized*  %(user)s@%(ip)s:/home/%(user)s/docker_host/hdfs/\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_cp = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_cp)  , shell=True, encoding='utf-8')\n",
    "    print(\"Keys deployed at \"+ row['hostname'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy HDFS configs to all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs deployed at pi-node4\n",
      "Configs deployed at pi-node5\n",
      "Configs deployed at pi-node7\n",
      "Configs deployed at pi-node8\n"
     ]
    }
   ],
   "source": [
    "for index, row in raspi_hosts.iterrows():\n",
    "    command_rmdir = \"ssh %(user)s@%(ip)s rm -rf /home/%(user)s/docker_host/hdfs/conf\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_rmdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_rmdir)  , shell=True, encoding='utf-8')\n",
    "    command_mkdir = \"ssh %(user)s@%(ip)s mkdir -p /home/%(user)s/docker_host/hdfs/conf\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_mkdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_mkdir)  , shell=True, encoding='utf-8')\n",
    "    command_cp = \"scp ~/repos/hdfs-sample/hdfs-conf/*  %(user)s@%(ip)s:/home/%(user)s/docker_host/hdfs/conf/\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result_cp = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_cp)  , shell=True, encoding='utf-8')\n",
    "    command = \"ssh %(user)s@%(ip)s ls /home/%(user)s/docker_host/hdfs/conf\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command)  , shell=True, encoding='utf-8')\n",
    "    print(\"Configs deployed at \"+ row['hostname'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namenode Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Namenode Stopped\n"
     ]
    }
   ],
   "source": [
    "for index, row in raspi_name_hosts.iterrows():\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker-compose -f ~/repos/hdfs-sample/docker-compose-namenode.yaml down\"  \n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "    print(\"HDFS Namenode Stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x 2 pi pi 4096 Jan 28 20:26 .\n",
      "drwxr-xr-x 4 pi pi 4096 Jan 28 20:26 ..\n",
      "\n",
      "HDFS Namenode Formated\n"
     ]
    }
   ],
   "source": [
    "# Create shared folder\n",
    "# Create Format disk\n",
    "# This is a destructive action(old data will be removed)\n",
    "if format_namenode:    \n",
    "    for index, row in raspi_name_hosts.iterrows():\n",
    "        docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "        command_rmdir = \"ssh %(user)s@%(ip)s sudo rm -rf /home/%(user)s/docker_host/persisted/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "        result_rmdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_rmdir)  , shell=True, encoding='utf-8')    \n",
    "        command_lsdir = \"ssh %(user)s@%(ip)s ls -la /home/%(user)s/docker_host/persisted/\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "        result_lsdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_lsdir)  , shell=True, encoding='utf-8')    \n",
    "        print(result_lsdir)\n",
    "        command_mkdir = \"ssh %(user)s@%(ip)s mkdir -p /home/%(user)s/docker_host/persisted/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "        result_mkdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_mkdir)  , shell=True, encoding='utf-8')    \n",
    "        command = \"docker run  -v /home/pi/docker_host/hdfs/id_rsa:/root/.ssh/id_rsa -v /home/pi/docker_host/hdfs/id_rsa.pub:/root/.ssh/id_rsa.pub  -v /home/pi/docker_host/hdfs/conf:/opt/hadoop/etc/hadoop -v /home/pi/docker_host/persisted/hdfs:/data/dfs/name local-hadoop:3.2.1 /opt/hadoop/bin/hdfs namenode -format\" % {\"user\":row['user'], \"ip\": row['ip']}   \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(\"HDFS Namenode Formated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Namenode Started\n"
     ]
    }
   ],
   "source": [
    "#Start name node\n",
    "for index, row in raspi_name_hosts.iterrows():\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker-compose -f ~/repos/hdfs-sample/docker-compose-namenode.yaml up -d\"  \n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "    print(\"HDFS Namenode Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datanodes Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mpi-node5\u001b[0m\n",
      "  Datanode data clean:pi-node5\n",
      "  Datanode started:pi-node5\n",
      "\u001b[1;35mpi-node7\u001b[0m\n",
      "  Datanode data clean:pi-node7\n",
      "  Datanode started:pi-node7\n"
     ]
    }
   ],
   "source": [
    "namenode_ip = raspi_name_hosts.iloc[0]['ip']\n",
    "namenode_config = 'hadoop-namenode:%(namenode_ip)s' %  {\"namenode_ip\": namenode_ip}\n",
    "hosts_list = [namenode_config]\n",
    "\n",
    "for index, row in raspi_data_hosts.iterrows():\n",
    "    hostname='hdfs-data-' + row['hostname']\n",
    "    datanode_config='%(hostname)s:%(ip)s' %  {\"hostname\": hostname,\"ip\": row['ip'] }\n",
    "    hosts_list.append(datanode_config)\n",
    "\n",
    "node_count = 0\n",
    "with open('/root/repos/hdfs-sample/docker-compose-datanode.yaml',\"r\") as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    namenode_config = 'hadoop-namenode:%(namenode_ip)s' %  {\"namenode_ip\": namenode_ip}\n",
    "    docker_host_config = hosts_list\n",
    "    config['services']['hadoop-datanode']['extra_hosts'] = docker_host_config\n",
    "    for index, row in raspi_data_hosts.iterrows():\n",
    "        node_count = node_count + 1\n",
    "        config['services']['hadoop-datanode']['hostname'] = 'hdfs-data-' + row['hostname']\n",
    "        with open(r'docker-compose-datanode-current.yaml', 'w') as outputfile:\n",
    "            documents = yaml.dump(config, outputfile)    \n",
    "        print('\\x1b[1;35m'+ row['hostname']+'\\x1b[0m')\n",
    "        docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "        command = \"docker-compose -f docker-compose-datanode-current.yaml down\"\n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        if clean_datanode: \n",
    "            command_rmdir = \"ssh %(user)s@%(ip)s sudo rm -rf /home/%(user)s/docker_host/persisted/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "            result_rmdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_rmdir)  , shell=True, encoding='utf-8')    \n",
    "            command_mkdir = \"ssh %(user)s@%(ip)s mkdir -p /home/%(user)s/docker_host/persisted/hdfs\" % {\"user\":row['user'], \"ip\": row['ip']}\n",
    "            result_mkdir = subprocess.check_output(\"bash -c \\\"%s  2>&1 || : \\\" \" %(command_mkdir)  , shell=True, encoding='utf-8')\n",
    "            print(\"  Datanode data clean:\" + row['hostname'])\n",
    "        command = \"docker-compose -f docker-compose-datanode-current.yaml up -d\"  \n",
    "        result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')        \n",
    "        print(\"  Datanode started:\" + row['hostname'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cluster post deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mpi-node4\u001b[0m\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\n",
      "5dc6b94328d9        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   2 weeks ago         Up 2 days           0.0.0.0:9100->9100/tcp   nodeexporter\n",
      "\n",
      "\u001b[1;35mpi-node5\u001b[0m\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                                                NAMES\n",
      "6f503d25c2f6        local-hadoop:3.2.1           \"/opt/hadoop/bin/hdf…\"   14 seconds ago      Up 11 seconds       22/tcp, 8020/tcp, 8088/tcp, 0.0.0.0:50010->50010/tcp, 9000/tcp, 0.0.0.0:50020->50020/tcp, 50030/tcp, 0.0.0.0:50060->50060/tcp, 50070/tcp, 50090/tcp, 0.0.0.0:50075->50075/tcp, 50470/tcp, 0.0.0.0:50475->50475/tcp   09-hdfs-on-arm-pi_hadoop-datanode_1\n",
      "ce4abe4927a6        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   2 weeks ago         Up 2 days           0.0.0.0:9100->9100/tcp                                                                                                                                                                                               nodeexporter\n",
      "\n",
      "\u001b[1;35mpi-node7\u001b[0m\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                                                NAMES\n",
      "c24e937f25c6        local-hadoop:3.2.1           \"/opt/hadoop/bin/hdf…\"   5 seconds ago       Up 1 second         22/tcp, 8020/tcp, 8088/tcp, 0.0.0.0:50010->50010/tcp, 9000/tcp, 0.0.0.0:50020->50020/tcp, 50030/tcp, 0.0.0.0:50060->50060/tcp, 50070/tcp, 50090/tcp, 0.0.0.0:50075->50075/tcp, 50470/tcp, 0.0.0.0:50475->50475/tcp   09-hdfs-on-arm-pi_hadoop-datanode_1\n",
      "72289a2a58bc        prom/prometheus:v2.12.0      \"/bin/prometheus --s…\"   2 weeks ago         Up 46 hours         0.0.0.0:9090->9090/tcp                                                                                                                                                                                               prometheus\n",
      "f94621b76cf6        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   2 weeks ago         Up 2 days           0.0.0.0:9100->9100/tcp                                                                                                                                                                                               nodeexporter\n",
      "\n",
      "\u001b[1;35mpi-node8\u001b[0m\n",
      "CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                                 NAMES\n",
      "a32f9495b0fe        local-hadoop:3.2.1           \"/opt/hadoop/bin/hdf…\"   25 seconds ago      Up 21 seconds       22/tcp, 8020/tcp, 8088/tcp, 50010/tcp, 50020/tcp, 50030/tcp, 0.0.0.0:9000->9000/tcp, 0.0.0.0:50070->50070/tcp, 50060/tcp, 50090/tcp, 50470/tcp, 50475/tcp, 0.0.0.0:50075->50075/tcp   hdfs-sample_hadoop-namenode_1\n",
      "e9fbb0da5d92        prom/node-exporter:v0.18.0   \"/bin/node_exporter …\"   2 weeks ago         Up 2 days           0.0.0.0:9100->9100/tcp                                                                                                                                                                nodeexporter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in raspi_hosts.iterrows():\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker ps\"    \n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s  || : \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print('\\x1b[1;35m'+ row['hostname']+'\\x1b[0m')\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test application to validate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add file to HDFS:\n",
      "\n",
      "Check file on HDFS:\n",
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup        775 2020-01-29 01:27 hdfs://hadoop-namenode:9000/hdfs-site.xml\n",
      "\n",
      "Open File to HDFS:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "\n",
      "</configuration>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_host_list=\" \"    \n",
    "for host in hosts_list: \n",
    "    add_host_list = add_host_list +\" --add-host=\" +host + \"  \"\n",
    "    \n",
    "    \n",
    "for index, row in raspi_test_hosts.iterrows():\n",
    "    print(\"Add file to HDFS:\")\n",
    "    docker_host = 'source /root/common/env.sh && export DOCKER_HOST=\\\"tcp://%(ip)s:2376\\\"' %  {\"ip\": row['ip']}\n",
    "    command = \"docker run  %(hostlist)s local-hadoop:3.2.1 /opt/hadoop/bin/hadoop fs -put /opt/hadoop/etc/hadoop/hdfs-site.xml  hdfs://hadoop-namenode:9000/ \" % {\"hostlist\": add_host_list}\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s || :   \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print(result)\n",
    "    print(\"Check file on HDFS:\")\n",
    "    command = \"docker run %(hostlist)s  local-hadoop:3.2.1 /opt/hadoop/bin/hadoop fs -ls  hdfs://hadoop-namenode:9000/ \" % {\"hostlist\": add_host_list}\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s || :  \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print(result)\n",
    "    print(\"Open File to HDFS:\")\n",
    "    command = \"docker run  %(hostlist)s local-hadoop:3.2.1 /opt/hadoop/bin/hadoop fs -cat hdfs://hadoop-namenode:9000/hdfs-site.xml\" % {\"hostlist\": add_host_list}\n",
    "    result = subprocess.check_output(\"bash -c \\\"%s && %s || :   \\\" \" %(docker_host,command)  , shell=True, encoding='utf-8')\n",
    "    print(result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
